# CLAUDE.md

## Project Overview

**AI Model Observatory 2026** is a real-time interactive dashboard for comparing and tracking global AI models across multiple categories (LLMs, image generation, video generation, voice/TTS) along with country-level AI capability rankings. The project is designed for easy deployment by non-technical users.

Primary language for comments and documentation is **Korean**.

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React 18 + Tailwind CSS (CDN-based, no build step) |
| Data Collection | Python 3.11 (`scrape_data.py`) |
| Data Storage | Static JSON file (`data.json`) |
| Web Server | Nginx (Alpine) |
| Containerization | Docker + Docker Compose |
| CI/CD | GitHub Actions (daily scheduled data updates) |
| Caching | Redis 7 (optional) |

## Repository Structure

```
ai_index_suncity/
├── index.html              # Main React dashboard (single-file SPA)
├── scrape_data.py          # Python data scraper (Artificial Analysis API v2)
├── data.json               # Generated data consumed by frontend
├── Dockerfile              # Backend container (python:3.11-slim + uvicorn)
├── docker-compose.yml      # 3-service orchestration (backend, nginx, redis)
├── nginx.conf              # Web server config with security headers + CORS
├── railway.json            # Railway.app deployment config
├── render.yaml             # Render.com deployment config
├── update-data.yml         # GitHub Actions workflow (also at .github/workflows/)
├── .github/workflows/
│   └── update-data.yml     # Daily data update cron (00:00 UTC)
├── DEPLOYMENT.md           # Comprehensive multi-platform deployment guide
├── GITHUB-DEPLOY-GUIDE.md  # GitHub-specific deployment instructions
├── AUTO-UPDATE-GUIDE.md    # Automated data updates setup
└── README-SIMPLE.md        # Quick start guide for non-technical users
```

## Key Architecture Decisions

- **No build step for frontend**: `index.html` uses CDN-hosted React, Tailwind, and Babel for in-browser JSX transpilation. No npm/webpack/vite needed.
- **No traditional database**: `data.json` is the sole data source. It is generated by `scrape_data.py` and served statically.
- **Backend note**: `Dockerfile` and configs reference `backend_api.py` (uvicorn-based API server), but this file does not currently exist in the repository. The project works in static-only mode via `index.html` + `data.json`.
- **No `requirements.txt`**: Referenced in Dockerfile but missing. Python dependencies are `requests` and `beautifulsoup4`.

## Data Flow

1. GitHub Actions triggers `scrape_data.py` daily at 00:00 UTC
2. Script fetches from Artificial Analysis API v2 endpoints (`llms/models`, `media/text-to-image`, `media/text-to-video`, `media/text-to-speech`)
3. Data is mapped/transformed and written to `data.json`
4. Changes are auto-committed by `github-actions[bot]`
5. Frontend fetches `data.json` on page load (with cache-busting query param)

## Environment Variables

| Variable | Purpose | Where Used |
|----------|---------|------------|
| `AI_MODELS_KEY` | Artificial Analysis API key | GitHub Actions secret, `scrape_data.py` |
| `PYTHONUNBUFFERED` | Direct Python stdout logging | Docker Compose |
| `LOG_LEVEL` | Logging verbosity | Docker Compose |

## Development Workflows

### Running the frontend locally

Open `index.html` directly in a browser. It fetches `data.json` from the same directory. No server required for basic viewing.

### Updating data manually

```bash
export AI_MODELS_KEY="your-api-key"
python3 scrape_data.py
```

This regenerates `data.json` with fresh API data.

### Running with Docker

```bash
docker-compose up --build
```

Starts backend (port 8000), nginx (port 80), and redis (port 6379). Note: requires `backend_api.py` and `requirements.txt` to exist.

## Code Conventions

### JavaScript / React (index.html)
- Functional components with hooks (`useState`, `useEffect`)
- camelCase for variables and functions
- Tailwind utility classes + custom CSS with glassmorphism patterns
- Color scheme: cyan `#00fff2`, purple `#a78bfa`, pink `#f472b6`

### Python (scrape_data.py)
- snake_case for variables and functions
- Korean comments and print statements
- Error handling with try/except on all network calls
- Top 15 models per category
- Safe extraction patterns with `.get()` and fallback defaults

### Documentation
- Written primarily in Korean
- Multiple deployment guides targeting non-technical users

## Data Schema

`data.json` contains these top-level keys:

- `llmModels[]` / `imageModels[]` / `videoModels[]` / `voiceModels[]` - Arrays of model objects with: `rank`, `name`, `company`, `score`, `price`, `usage`, `color`, `url`, `isKorean`, `specialty`
- `countries[]` - Country ranking objects with: `rank`, `name`, `flag`, `aiPower`, `investment`, `adoption`, `models`, `trend`
- `insights[]` - Dashboard insight cards with: `title`, `description`, `icon`, `color`
- `lastUpdate` - ISO 8601 timestamp
- `metadata` - Version and source info

## Testing

No formal test suite exists. There are no unit tests, integration tests, or test configuration files.

## Deployment Targets

Supported platforms (documented in `DEPLOYMENT.md`):
- **Railway.app** (recommended, $5/month free credit)
- **Render.com** (free tier available)
- **AWS EC2** (manual Docker setup)
- **Google Cloud Run** (serverless)
- **Vercel** (frontend-only)
- **Traditional VPS** (DigitalOcean, Linode, Vultr)

## Common Pitfalls

- `backend_api.py` and `requirements.txt` are referenced in Docker/deployment configs but do not exist in the repo. Docker builds will fail without them.
- `docker-compose.yml` mounts `frontend-api-connected.html` but the actual frontend file is `index.html`.
- The `AI_MODELS_KEY` environment variable must be set for `scrape_data.py` to fetch data successfully.
- `data.json` is committed to the repo and auto-updated by CI. Avoid manual edits as they will be overwritten.
