#!/usr/bin/env python3
import json
import requests
import os
from datetime import datetime

# ê³µí†µ ì„¤ì •
API_BASE_URL = "https://artificialanalysis.ai/api/v2/data"
# [ì£¼ì˜] ê¹ƒí—ˆë¸Œì— ì˜¬ë¦´ ë•ŒëŠ” ë³´ì•ˆì„ ìœ„í•´ ì•„ëž˜ ì¤„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
# API_KEY = os.environ.get('AI_MODELS_KEY')
API_KEY = "aa_nlHXrHmYGAApxkFnjBrBFcYPegOsmqKZ"

# í•œêµ­ ê¸°ì—… ëª©ë¡ ë¡œë“œ
def load_korean_companies():
    try:
        with open('korean_companies.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('companies', [])
    except Exception as e:
        print(f"âš ï¸ korean_companies.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return ["Naver", "Wrtn", "Upstage", "Kakao", "Samsung", "LG", "SK Telecom", "KT"]

def fetch_api_data(endpoint):
    """API ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê³µí†µ í•¨ìˆ˜"""
    url = f"{API_BASE_URL}/{endpoint}"
    headers = {"x-api-key": API_KEY, "Content-Type": "application/json"}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            return response.json().get('data', [])
        else:
            print(f"âŒ API ì˜¤ë¥˜ ({endpoint}): {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ({endpoint}): {e}")
        return []

def is_korean_company(creator_name, korean_list):
    """í•œêµ­ ê¸°ì—… ì—¬ë¶€ í™•ì¸"""
    return any(kr.lower() in creator_name.lower() for kr in korean_list)

def map_llm_data(items, korean_list):
    """LLM ëª¨ë¸ ë°ì´í„° ë§¤í•‘ (ìƒìœ„ 20ê°œ, í’ë¶€í•œ í‰ê°€ ë°ì´í„° í¬í•¨)"""
    mapped_list = []
    for index, item in enumerate(items[:20]):
        creator = item.get('model_creator', {})
        creator_name = creator.get('name', 'Unknown')
        evals = item.get('evaluations', {})

        # ì ìˆ˜ ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì‹œë„)
        score = evals.get('artificial_analysis_intelligence_index', 0)
        coding_score = (
            evals.get('artificial_analysis_coding_index', 0) or
            evals.get('coding', 0) or
            evals.get('humaneval', 0) or 0
        )
        math_score = (
            evals.get('artificial_analysis_math_index', 0) or
            evals.get('math', 0) or
            evals.get('gsm8k', 0) or 0
        )

        # ì†ë„ (í† í°/ì´ˆ)
        speed = item.get('median_output_tokens_per_second', 0) or 0

        # ê°€ê²© ($/ë°±ë§Œ í† í°)
        input_price = item.get('input_cost_per_million', 0) or 0
        output_price = item.get('output_cost_per_million', 0) or 0

        # ê°€ì„±ë¹„ ì ìˆ˜: ì¢…í•©ì ìˆ˜ / ì¶œë ¥ê°€ê²© * 10 (ê°€ê²©ì´ 0ì´ë©´ ì ìˆ˜ ê·¸ëŒ€ë¡œ)
        if output_price > 0:
            value_score = round(score / output_price * 10, 2)
        else:
            value_score = round(score * 100, 2)

        mapped_list.append({
            "name": item.get('name', 'Unknown'),
            "company": creator_name,
            "score": score,
            "codingScore": coding_score,
            "mathScore": math_score,
            "speed": speed,
            "inputPrice": input_price,
            "outputPrice": output_price,
            "valueScore": value_score,
            "color": "#00fff2" if index < 3 else "#a78bfa",
            "url": f"https://artificialanalysis.ai/models/{item.get('slug', '')}",
            "isKorean": is_korean_company(creator_name, korean_list)
        })
    return mapped_list

def map_media_data(items, korean_list):
    """ë¯¸ë””ì–´ ëª¨ë¸ ë°ì´í„° ë§¤í•‘ (ìƒìœ„ 20ê°œ)"""
    mapped_list = []
    for index, item in enumerate(items[:20]):
        creator = item.get('model_creator', {})
        creator_name = creator.get('name', 'Unknown')

        categories = item.get('categories', [])
        specialty = categories[0].get('style_category', 'General') if categories else "General"

        score = item.get('elo', 0)
        num_ratings = item.get('num_ratings', 0) or item.get('total_votes', 0) or 0
        release_date = item.get('release_date', '') or ''

        mapped_list.append({
            "name": item.get('name', 'Unknown'),
            "company": creator_name,
            "score": score,
            "specialty": specialty,
            "numRatings": num_ratings,
            "releaseDate": release_date,
            "color": "#00fff2" if index < 3 else "#a78bfa",
            "url": f"https://artificialanalysis.ai/models/{item.get('slug', '')}",
            "isKorean": is_korean_company(creator_name, korean_list)
        })
    return mapped_list

def main():
    print("ðŸš€ AI Model Observatory í†µí•© ë°ì´í„° ìˆ˜ì§‘ ì‹œìž‘\n")

    korean_list = load_korean_companies()
    print(f"ðŸ‡°ðŸ‡· í•œêµ­ ê¸°ì—… ëª©ë¡: {len(korean_list)}ê°œ ë¡œë“œë¨")

    # 1. LLM ëª¨ë¸ ìˆ˜ì§‘
    print("\nðŸ§  LLM ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    llm_raw = fetch_api_data("llms/models")
    llm_models = map_llm_data(llm_raw, korean_list)

    # 2. Text-to-Image ëª¨ë¸ ìˆ˜ì§‘
    print("ðŸŽ¨ Text-to-Image ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    tti_raw = fetch_api_data("media/text-to-image")
    tti_models = map_media_data(tti_raw, korean_list)

    # 3. Text-to-Speech ëª¨ë¸ ìˆ˜ì§‘
    print("ðŸŽ™ï¸ Text-to-Speech ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    tts_raw = fetch_api_data("media/text-to-speech")
    tts_models = map_media_data(tts_raw, korean_list)

    # 4. Text-to-Video ëª¨ë¸ ìˆ˜ì§‘
    print("ðŸŽ¬ Text-to-Video ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    ttv_raw = fetch_api_data("media/text-to-video")
    ttv_models = map_media_data(ttv_raw, korean_list)

    # 5. Image-to-Video ëª¨ë¸ ìˆ˜ì§‘
    print("ðŸŽžï¸ Image-to-Video ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    itv_raw = fetch_api_data("media/image-to-video")
    itv_models = map_media_data(itv_raw, korean_list)

    # JSON ë°ì´í„° ìƒì„±
    data = {
        "llmModels": llm_models,
        "textToImageModels": tti_models,
        "textToSpeechModels": tts_models,
        "textToVideoModels": ttv_models,
        "imageToVideoModels": itv_models,
        "lastUpdate": datetime.now().isoformat(),
        "metadata": {
            "version": "3.0",
            "source": "Artificial Analysis API v2",
            "apiEndpoints": {
                "llm": "https://artificialanalysis.ai/api/v2/data/llms/models",
                "textToImage": "https://artificialanalysis.ai/api/v2/data/media/text-to-image",
                "textToSpeech": "https://artificialanalysis.ai/api/v2/data/media/text-to-speech",
                "textToVideo": "https://artificialanalysis.ai/api/v2/data/media/text-to-video",
                "imageToVideo": "https://artificialanalysis.ai/api/v2/data/media/image-to-video"
            }
        }
    }

    total = len(llm_models) + len(tti_models) + len(tts_models) + len(ttv_models) + len(itv_models)
    korean_count = sum(1 for m in llm_models + tti_models + tts_models + ttv_models + itv_models if m.get('isKorean'))

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… data.json ìƒì„± ì™„ë£Œ!")
    print(f"   ðŸ“Š ì´ {total}ê°œ ëª¨ë¸ (LLM:{len(llm_models)} TTI:{len(tti_models)} TTS:{len(tts_models)} TTV:{len(ttv_models)} ITV:{len(itv_models)})")
    print(f"   ðŸ‡°ðŸ‡· í•œêµ­ ê¸°ì—… ëª¨ë¸: {korean_count}ê°œ")

if __name__ == "__main__":
    main()
