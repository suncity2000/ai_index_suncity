#!/usr/bin/env python3
"""
AI Model Observatory - ìë™ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ AI ëª¨ë¸ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ data.json ìƒì„±
"""

import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

def scrape_artificialanalysis():
    """Artificial Analysisì—ì„œ LLM ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
    print("ğŸ“Š Artificial Analysis ìŠ¤í¬ë˜í•‘ ì¤‘...")
    
    # ì‹¤ì œë¡œëŠ” APIë‚˜ ì›¹ ìŠ¤í¬ë˜í•‘
    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ê°„ë‹¨í•œ ë²„ì „
    
    try:
        # ì˜ˆì‹œ: Artificial Analysis í˜ì´ì§€ ì ‘ê·¼
        url = "https://artificialanalysis.ai/leaderboards/models"
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ë¡œì§ì€ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„° ë°˜í™˜
        
        #models = [
          #  {"rank": 1, "name": "Claude Opus 4.5", "company": "Anthropic", "score": 89.4, "price": "$$$", "usage": 95, "color": "#00fff2", "url": "https://claude.ai", "isKorean": False, "newFeatures": ["Extended Context", "Vision"]},
           # {"rank": 2, "name": "GPT-5", "company": "OpenAI", "score": 88.0, "price": "$$$$", "usage": 92, "color": "#a78bfa", "url": "https://openai.com/gpt-5", "isKorean": False, "newFeatures": ["Multimodal", "Long Context"]},
        #]
        
        print(f"âœ… LLM ëª¨ë¸ {len(models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return models
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")
        return []

def scrape_lmsys_arena():
    """LMSYS Arenaì—ì„œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ëª¨ë¸ ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
    print("ğŸ¨ LMSYS Arena ìŠ¤í¬ë˜í•‘ ì¤‘...")
    
    try:
        # Arena AI ì´ë¯¸ì§€ ë¦¬ë”ë³´ë“œ
        url = "https://huggingface.co/spaces/LMSYS/arena-leaderboard"
        
        # ì‹¤ì œ êµ¬í˜„ ì‹œ API ì‚¬ìš© ë˜ëŠ” ìŠ¤í¬ë˜í•‘
        
        image_models = [
            {"rank": 1, "name": "Nano Banana Pro", "company": "Google", "score": 95, "price": "$$", "usage": 92, "specialty": "Photorealism", "color": "#f472b6", "url": "https://labs.google/nano", "isKorean": False},
        ]
        
        print(f"âœ… ì´ë¯¸ì§€ ëª¨ë¸ {len(image_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return image_models
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")
        return []

def scrape_voice_models():
    """ìŒì„±/TTS ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸ™ï¸ ìŒì„± ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    voice_models = [
        {"rank": 1, "name": "ElevenLabs Turbo v3", "company": "ElevenLabs", "score": 96, "price": "$$", "usage": 94, "specialty": "ìì—°ìŠ¤ëŸ¬ìš´ ì–µì–‘", "color": "#a78bfa", "url": "https://elevenlabs.io", "isKorean": False},
        {"rank": 3, "name": "Clova Dubbing", "company": "Naver", "score": 91, "price": "$$", "usage": 78, "specialty": "í•œêµ­ì–´ íŠ¹í™”", "color": "#10b981", "url": "https://clovadubbing.naver.com", "isKorean": True},
    ]
    
    print(f"âœ… ìŒì„± ëª¨ë¸ {len(voice_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return voice_models

def scrape_agent_services():
    """AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸ¤– AI ì—ì´ì „íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    agent_models = [
        {"rank": 1, "name": "Genspark", "company": "Genspark", "score": 93, "price": "Free", "usage": 89, "specialty": "AI ê²€ìƒ‰ ì—”ì§„", "color": "#a78bfa", "url": "https://genspark.ai", "isKorean": False},
        {"rank": 4, "name": "Wrtn Search", "company": "Wrtn Technologies", "score": 88, "price": "Free", "usage": 74, "specialty": "í•œêµ­ì–´ AI ê²€ìƒ‰", "color": "#10b981", "url": "https://wrtn.ai", "isKorean": True},
    ]
    
    print(f"âœ… AI ì—ì´ì „íŠ¸ {len(agent_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return agent_models

def scrape_stanford_hai():
    """Stanford HAIì—ì„œ êµ­ê°€ë³„ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸŒ Stanford HAI êµ­ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    countries = [
        {"rank": 1, "name": "ë¯¸êµ­", "flag": "ğŸ‡ºğŸ‡¸", "aiPower": 39700, "investment": 108.5, "adoption": 28.3, "models": 561, "trend": "up"},
        {"rank": 2, "name": "ì¤‘êµ­", "flag": "ğŸ‡¨ğŸ‡³", "aiPower": 400, "investment": 63.3, "adoption": 18.5, "models": 342, "trend": "up"},
        {"rank": 6, "name": "í•œêµ­", "flag": "ğŸ‡°ğŸ‡·", "aiPower": 5700, "investment": 11.8, "adoption": 42.1, "models": 78, "trend": "up"},
    ]
    
    print(f"âœ… êµ­ê°€ ë°ì´í„° {len(countries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return countries

def generate_insights():
    """ì¸ì‚¬ì´íŠ¸ ë°ì´í„° ìƒì„±"""
    insights = [
        {"title": "ë©€í‹°ëª¨ë‹¬ í†µí•© ê°€ì†í™”", "description": "2025ë…„ AI ëª¨ë¸ë“¤ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í™”.", "icon": "ğŸ”„", "color": "border-cyan-400"},
        {"title": "í•œêµ­ AI ì„œë¹„ìŠ¤ ì•½ì§„", "description": "Clova Dubbing, Wrtn, Typecast ë“± í•œêµ­ AI ì„œë¹„ìŠ¤ë“¤ì´ ê¸€ë¡œë²Œ í†±10 ì§„ì….", "icon": "ğŸ‡°ğŸ‡·", "color": "border-blue-400"},
    ]
    return insights

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AI Model Observatory ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\n")
    
    # ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
    llm_models = scrape_artificialanalysis()
    image_models = scrape_lmsys_arena()
    voice_models = scrape_voice_models()
    agent_models = scrape_agent_services()
    countries = scrape_stanford_hai()
    insights = generate_insights()
    
    # ë¹„ë””ì˜¤ ëª¨ë¸ (ì„ì‹œ)
    video_models = [
        {"rank": 1, "name": "Veo 3.1", "company": "Google", "score": 94, "price": "$$$", "usage": 88, "duration": "8s with audio", "color": "#f472b6", "url": "https://deepmind.google/technologies/veo/", "isKorean": False},
    ]
    
    # JSON ë°ì´í„° ìƒì„±
    data = {
        "llmModels": llm_models,
        "imageModels": image_models,
        "videoModels": video_models,
        "voiceModels": voice_models,
        "agentModels": agent_models,
        "countries": countries,
        "insights": insights,
        "lastUpdate": datetime.now().isoformat(),
        "metadata": {
            "version": "1.0",
            "source": "Automated scraping",
            "nextUpdate": "Next update in 24 hours"
        }
    }
    
    # data.json íŒŒì¼ë¡œ ì €ì¥
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… data.json íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ì´ {len(llm_models)} LLM, {len(image_models)} ì´ë¯¸ì§€, {len(voice_models)} ìŒì„±, {len(agent_models)} ì—ì´ì „íŠ¸ ëª¨ë¸ ìˆ˜ì§‘")

if __name__ == "__main__":
    main()
