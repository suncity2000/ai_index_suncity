#!/usr/bin/env python3
"""
AI Model Observatory - ìë™ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ AI ëª¨ë¸ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ data.json ìƒì„±
"""

import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

import os  # ê¹ƒí—ˆë¸Œì— ì €ì¥í•œ í‚¤ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìš”í•´ìš”
from datetime import datetime

def scrape_artificialanalysis():
    print("ğŸ“Š Artificial Analysis APIì—ì„œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    # 1. ê¹ƒí—ˆë¸Œ Secretsì— ì €ì¥í•œ API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    #api_key = os.environ.get('AI_MODELS_KEY')
    api_key = "aa_nlHXrHmYGAApxkFnjBrBFcYPegOsmqKZ"
    # API ì£¼ì†Œ (ë¬¸ì„œì— ëª…ì‹œëœ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”)
    url = "https://artificialanalysis.ai/api/v2/data/llms/models" 
    response = requests.get(url, headers={"x-api-key": api_key})
    print(response.json()) # ì´ ì¤„ì´ í•µì‹¬ì…ë‹ˆë‹¤!
    headers = {
        "x-api-key": api_key,
        "Content-Type": "application/json"
    }
    
    try:
        # 2. ì‹¤ì œ API í˜¸ì¶œ
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            api_data = response.json()
            
            # 3. API ë°ì´í„°ë¥¼ ìš°ë¦¬ ì›¹ì‚¬ì´íŠ¸(index.html) í˜•ì‹ì— ë§ê²Œ ë³€í™˜
            models = []
            # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ë°˜ë³µë¬¸ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
            for index, item in enumerate(api_data.get('models', [])[:10]): # ìƒìœ„ 10ê°œë§Œ
                models.append({
                    "rank": index + 1,
                    "name": item.get('name'),
                    "company": item.get('creator_name'),
                    "score": item.get('intelligence_score', 0), # ì§€ëŠ¥ ì ìˆ˜
                    "price": "$$$" if item.get('pricing_type') == 'usage' else "$",
                    "usage": 90, # ì„ì‹œê°’ (API ì œê³µ ì—¬ë¶€ì— ë”°ë¼ ìˆ˜ì •)
                    "color": "#00fff2" if index == 0 else "#a78bfa",
                    "url": f"https://artificialanalysis.ai/models/{item.get('slug')}",
                    "isKorean": False,
                    "newFeatures": ["API Real-time"]
                })
            
            print(f"âœ… ì‹¤ì œ ëª¨ë¸ {len(models)}ê°œ ìˆ˜ì§‘ ì„±ê³µ!")
            return models
        else:
            print(f"âŒ API ì˜¤ë¥˜ ë°œìƒ (ì½”ë“œ: {response.status_code})")
            return []

    except Exception as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
        return []

def scrape_lmsys_arena():
    """LMSYS Arenaì—ì„œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ëª¨ë¸ ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
    print("ğŸ¨ LMSYS Arena ìŠ¤í¬ë˜í•‘ ì¤‘...")
    
    try:
        # Arena AI ì´ë¯¸ì§€ ë¦¬ë”ë³´ë“œ
        url = "https://huggingface.co/spaces/LMSYS/arena-leaderboard"
        
        # ì‹¤ì œ êµ¬í˜„ ì‹œ API ì‚¬ìš© ë˜ëŠ” ìŠ¤í¬ë˜í•‘
        
        image_models = [
            {"rank": 1, "name": "Nano Banana Pro", "company": "Google", "score": 95, "price": "$$", "usage": 92, "specialty": "Photorealism", "color": "#f472b6", "url": "https://labs.google/nano", "isKorean": False},
        ]
        
        print(f"âœ… ì´ë¯¸ì§€ ëª¨ë¸ {len(image_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return image_models
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")
        return []

def scrape_voice_models():
    """ìŒì„±/TTS ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸ™ï¸ ìŒì„± ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    voice_models = [
        {"rank": 1, "name": "ElevenLabs Turbo v3", "company": "ElevenLabs", "score": 96, "price": "$$", "usage": 94, "specialty": "ìì—°ìŠ¤ëŸ¬ìš´ ì–µì–‘", "color": "#a78bfa", "url": "https://elevenlabs.io", "isKorean": False},
        {"rank": 3, "name": "Clova Dubbing", "company": "Naver", "score": 91, "price": "$$", "usage": 78, "specialty": "í•œêµ­ì–´ íŠ¹í™”", "color": "#10b981", "url": "https://clovadubbing.naver.com", "isKorean": True},
    ]
    
    print(f"âœ… ìŒì„± ëª¨ë¸ {len(voice_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return voice_models

def scrape_agent_services():
    """AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸ¤– AI ì—ì´ì „íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    agent_models = [
        {"rank": 1, "name": "Genspark", "company": "Genspark", "score": 93, "price": "Free", "usage": 89, "specialty": "AI ê²€ìƒ‰ ì—”ì§„", "color": "#a78bfa", "url": "https://genspark.ai", "isKorean": False},
        {"rank": 4, "name": "Wrtn Search", "company": "Wrtn Technologies", "score": 88, "price": "Free", "usage": 74, "specialty": "í•œêµ­ì–´ AI ê²€ìƒ‰", "color": "#10b981", "url": "https://wrtn.ai", "isKorean": True},
    ]
    
    print(f"âœ… AI ì—ì´ì „íŠ¸ {len(agent_models)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return agent_models

def scrape_stanford_hai():
    """Stanford HAIì—ì„œ êµ­ê°€ë³„ ë°ì´í„° ìˆ˜ì§‘"""
    print("ğŸŒ Stanford HAI êµ­ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    countries = [
        {"rank": 1, "name": "ë¯¸êµ­", "flag": "ğŸ‡ºğŸ‡¸", "aiPower": 39700, "investment": 108.5, "adoption": 28.3, "models": 561, "trend": "up"},
        {"rank": 2, "name": "ì¤‘êµ­", "flag": "ğŸ‡¨ğŸ‡³", "aiPower": 400, "investment": 63.3, "adoption": 18.5, "models": 342, "trend": "up"},
        {"rank": 6, "name": "í•œêµ­", "flag": "ğŸ‡°ğŸ‡·", "aiPower": 5700, "investment": 11.8, "adoption": 42.1, "models": 78, "trend": "up"},
    ]
    
    print(f"âœ… êµ­ê°€ ë°ì´í„° {len(countries)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    return countries

def generate_insights():
    """ì¸ì‚¬ì´íŠ¸ ë°ì´í„° ìƒì„±"""
    insights = [
        {"title": "ë©€í‹°ëª¨ë‹¬ í†µí•© ê°€ì†í™”", "description": "2025ë…„ AI ëª¨ë¸ë“¤ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í™”.", "icon": "ğŸ”„", "color": "border-cyan-400"},
        {"title": "í•œêµ­ AI ì„œë¹„ìŠ¤ ì•½ì§„", "description": "Clova Dubbing, Wrtn, Typecast ë“± í•œêµ­ AI ì„œë¹„ìŠ¤ë“¤ì´ ê¸€ë¡œë²Œ í†±10 ì§„ì….", "icon": "ğŸ‡°ğŸ‡·", "color": "border-blue-400"},
    ]
    return insights

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AI Model Observatory ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\n")
    
    # ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
    llm_models = scrape_artificialanalysis()
    image_models = scrape_lmsys_arena()
    voice_models = scrape_voice_models()
    agent_models = scrape_agent_services()
    countries = scrape_stanford_hai()
    insights = generate_insights()
    
    # ë¹„ë””ì˜¤ ëª¨ë¸ (ì„ì‹œ)
    video_models = [
        {"rank": 1, "name": "Veo 3.1", "company": "Google", "score": 94, "price": "$$$", "usage": 88, "duration": "8s with audio", "color": "#f472b6", "url": "https://deepmind.google/technologies/veo/", "isKorean": False},
    ]
    
    # JSON ë°ì´í„° ìƒì„±
    data = {
        "llmModels": llm_models,
        "imageModels": image_models,
        "videoModels": video_models,
        "voiceModels": voice_models,
        "agentModels": agent_models,
        "countries": countries,
        "insights": insights,
        "lastUpdate": datetime.now().isoformat(),
        "metadata": {
            "version": "1.0",
            "source": "Automated scraping",
            "nextUpdate": "Next update in 24 hours"
        }
    }
    
    # data.json íŒŒì¼ë¡œ ì €ì¥
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… data.json íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“… ì—…ë°ì´íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ì´ {len(llm_models)} LLM, {len(image_models)} ì´ë¯¸ì§€, {len(voice_models)} ìŒì„±, {len(agent_models)} ì—ì´ì „íŠ¸ ëª¨ë¸ ìˆ˜ì§‘")

if __name__ == "__main__":
    main()
